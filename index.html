<!doctype html>
<html lang="en">
    <head>
        <title>Capturing Stable HDR Videos Using a Dual-Camera System</title>
        <link rel="icon" type="image/x-icon" href="/DCS-HDR/static/img/icons/camera.ico">

        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <!-- Open Graph -->
        <meta property="og:url" content="https://zqqqyu.github.io/DCS-HDR" />
        <meta property="og:image" content="https://zqqqyu.github.io/DCS-HDR/static/img/preview.png" />
        <meta property="og:title" content="Capturing Stable HDR Videos Using a Dual-Camera System" />
        <meta property="og:description" content="The proposed pipeline achieves temporally stable, flicker-free HDR videos and remains compatible with existing HDR deghosting models." />

        <!-- Twitter -->
        <meta name="twitter:url" content="https://zqqqyu.github.io/DCS-HDR" />
        <meta name="twitter:card" content="summary_large_image" />
        <meta name="twitter:image" content="https://zqqqyu.github.io/DCS-HDR/static/img/preview.png" />
        <meta name="twitter:title" content="Capturing Stable HDR Videos Using a Dual-Camera System" />
        <meta name="twitter:description" content="The proposed pipeline achieves temporally stable, flicker-free HDR videos and remains compatible with existing HDR deghosting models." />

        <script src="./static/js/distill_template.v2.js"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>

        <script defer="" src="./static/js/hider.js"></script>
        <script src="./static/js/image_interact.js"></script>
        <script src="./static/js/switch_videos.js"></script>

        <link rel="stylesheet" href="./static/css/style.css">
        <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
        <script defer src="./static/js/fontawesome.all.min.js"></script>


        <!-- medium zoom https://github.com/francoischalifour/medium-zoom -->
        <script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>  <!-- jquery -->
        <script defer src="./static/js/medium-zoom.min.js"></script>
        <script defer src="./static/js/zoom.js"></script>

        <meta charset="utf-8">
        <title>Table Example</title>
        <style>
          .best {
            color: red;
            font-weight: bold;
          }
          .secondbest {
            color: blue;
            text-decoration: underline;
          }
        </style>
    </head>
    <body>
        <div class="header-wrapper">
            <div class="header-container" id="header-container">
                <div class="header-content">
                    <h1 style="margin-top: 0px"><i>Capturing Stable HDR Videos Using a Dual-Camera System</i></h1>


                        <div class="icon-container">
                            <div class="icon-item">
                                <img src="./static/img/icons/thinking.svg" alt="Visual Representation Icon">
                                <div><strong>Paradigm</strong>: We introduce a dual-stream HDR video generation paradigm that explicitly decouples temporal luminance anchoring from exposure-variant detail reconstruction.</div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/connector.svg" alt="Connector Design Icon">
                                <div><strong>System</strong>: We design and implement an asynchronous dual-camera system to validate the feasibility of our proposed solution and bridge the gap between algorithmic design and practical deployment.</div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/data.svg" alt="Instruction Tuning Data Icon">
                                <div><strong>Method</strong>: To support our dual-camera system, we propose a novel model design, EAFNet.</div>
                            </div>
                            <div class="icon-item">
                                <img src="./static/img/icons/eval.svg" alt="Instruction Tuning Recipes Icon">
                                <div><strong>Results</strong>: Our proposed dual-stream HDR video generation paradigm demonstrates significant advancements across multiple aspects.</div>
                            </div>
                        </div>

                    <div class="button-container">
                        <!-- replace arxiv -->
                        <a href="https://arxiv.org/abs/2507.06593" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="ai ai-arxiv"></i>
                            </span>
                            arXiv
                        </a>
                        <!-- replace pdf -->
                        <a href="https://arxiv.org/abs/2507.06593" class="button paper-link" target="_blank">
                            <span class="icon is-small">
                                <i class="fas fa-file-pdf"></i>
                            </span>
                            <span>pdf</span>
                        </a>
                        <!-- replace image -->
                        <a href="https://github.com/zqqqyu/DCS" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Code</span>
                        </a>
                        <!-- <br> -->
                        <a href="https://github.com/zqqqyu/DCS" class="button" target="_blank">
                            <span class="icon is-small">
                                <i class="fab fa-github"></i>
                            </span>
                            <span>Data</span>
                        </a>
                    </div>
                </div>
                <div class="header-image">
                    <img draggable="false" src="static/img/dualcamera2.jpg" alt="Teaser Image" class="teaser-image">
                </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <p>
                    <a href="https://github.com/zqqqyu" class="author-link" target="_blank">Qianyu Zhang<sup>1</sup></a> &emsp;
                    <a href="https://scholar.google.com/citations?user=cZRVzVYAAAAJ" class="author-link" target="_blank">Bolun Zheng<sup>1,*</sup></a> &emsp;
                    <a href="https://scholar.google.com/citations?user=IhyTEDkAAAAJ" class="author-link" target="_blank">Lingyu Zhu<sup>2</sup></a> &emsp;
                    <br>
                    <a href="https://zqqqyu.github.io/DCS-HDR/" class="author-link" target="_blank">Hangjia Pan<sup>1</sup></a> &emsp;
                    <a href="https://zqqqyu.github.io/DCS-HDR/" class="author-link" target="_blank">Zunjie Zhu<sup>1</sup></a> &emsp;
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=UnsBY_AAAAAJ" class="author-link" target="_blank">Zongpeng Li<sup>1</sup></a> &emsp;
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=Pr7s2VUAAAAJ" class="author-link" target="_blank">Shiqi Wang<sup>2</sup></a> &emsp;
                    <p></p>
                </p>
                <sup>1</sup>Hangzhou Dianzi University <img src="static/img/logo/hdu.png" alt="HDU logo" style="height: 1em; vertical-align: middle; margin-left: 0px; margin-right: 5px;">,
                <sup>2</sup>City University of HongKong <img src="static/img/logo/cityu.png" alt="TJU logo" style="height: 1em; vertical-align: middle; margin-left: 0px; margin-right: 5px;">,
                </p>
                <p style="text-align: center; margin-bottom: 0;">
                    <span class="author-note"><sup>*</sup>Corresponding author</span>
                </p>
            </div>
        </div>


        <p class="text abstract">

            We present a <strong>dual-camera HDR video paradigm</strong> that decouples temporal luminance anchoring from exposure-variant detail recovery. 
            In contrast to single-camera alternating-exposure pipelines that often suffer from flicker and ghosting in dynamic scenes, 
            our design uses a mid-exposure <em>reference stream</em> to stabilize temporal consistency and an <em>auxiliary stream</em> with alternating low/high exposures to supply extreme luminance details.

            <br><br>
            our contributions can be summarized as follows:
            <ol class="text">
                <li><strong><a href="#Paradigm">&sect;Paradigm</a></strong>: We introduce a dual-stream HDR video generation paradigm that explicitly decouples temporal luminance anchoring from exposure-variant detail reconstruction.</li>
                <li><strong><a href="#System">&sect;System</a></strong>: We design and implement an asynchronous dual-camera system to validate the feasibility of our proposed solution and bridge the gap between algorithmic design and practical deployment.</li>
                <li><strong><a href="#Method">&sect;Method</a></strong>: To support our dual-camera system, we propose a novel model design, EAFNet.</li>
                <li><strong><a href="#Results">&sect;Results</a></strong>: Our proposed dual-stream HDR video generation paradigm demonstrates significant advancements across multiple aspects.</li>
            </ol>
        </p>

        <div class="icon-row">
            <a href="#Paradigm" class="icon-link">
                <img src="static/img/icons/thinking.svg" alt="Visual Representation Logo" class="icon">
                Paradigm
            </a>
            <a href="#System" class="icon-link">
                <img src="static/img/icons/connector.svg" alt="Connector Logo" class="icon">
                System
            </a>
            <a href="#Method" class="icon-link">
                <img src="static/img/icons/data.svg" alt="Data Logo" class="icon">
                Method
            </a>
            <a href="#Results" class="icon-link">
                <img src="static/img/icons/eval.svg" alt="Recipe Logo" class="icon">
                Results
            </a>
        </div>

        <p class="click-hint" style="width: 85%;">
            <img src="static/img/icons/click.gif" style="width: 1.5rem">
            <strong>Click to jump to each section.</strong>
        </p>


        <p class="text abstract">
            The proposed pipeline achieves temporally stable, flicker-free HDR videos and remains compatible with existing HDR deghosting models. 
            We release <a href="https://github.com/zqqqyu/DCS" target="_blank">code</a>, <a href="https://github.com/zqqqyu/DCS" target="_blank">data</a>, and implementation details to facilitate adoption in real-world HDR video capture.
        </p>



        <hr>




        








        <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">

        <style>
            /* --- 全局基础样式 --- */
            body {
                font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
                color: #333;
                margin: 0; 
                padding: 0;
                /* 允许横向撑满，但防止出现滚动条 */
                overflow-x: hidden; 
            }
        
            .sub-section {
                margin-bottom: 80px;
                opacity: 0; 
                animation: fadeIn 1s ease-out forwards;
                padding: 0 10px;
            }
        
            @keyframes fadeIn {
                from { opacity: 0; transform: translateY(20px); }
                to { opacity: 1; transform: translateY(0); }
            }
        
            .text {
                text-align: center;
                max-width: 900px;
                margin: 0 auto 20px auto;
                line-height: 1.6;
                color: #555;
                font-size: 1.1rem;
            }
            
            h1.text {
                font-size: 2.5rem;
                color: #111;
                margin-bottom: 15px;
                font-weight: 700;
                letter-spacing: -0.5px;
            }
        
            /* --- 视频大容器 (关键修改：超级宽) --- */
/* --- 视频大容器 (关键修改：强制撑满屏幕) --- */
            .video-container-card {
                background: #fff;
                border-radius: 16px;
                box-shadow: 0 10px 40px rgba(0,0,0,0.08);
                padding: 30px 20px; /* 上下30px，左右20px */
                
                /* === 核心修改开始 === */
                
                /* 1. 宽度设为视口(屏幕)宽度的 95% */
                width: 95vw; 
                
                /* 2. 忽略父容器限制，强制居中 */
                position: relative;
                left: 50%;
                right: 50%;
                margin-left: -47.5vw; /* 宽度的一半 (95/2) */
                margin-right: -47.5vw;
                
                /* 3. 限制最大宽度，防止在超宽带鱼屏上太夸张 */
                max-width: 1920px; 
                
                /* === 核心修改结束 === */
                
                box-sizing: border-box; 
            }
        
            /* 视频网格布局 */
           /* 视频网格布局 */
           .video-grid {
                display: flex;
                flex-wrap: wrap;
                justify-content: center;
                gap: 10px; /* 间隙调小 */
                width: 100%; /* 确保撑满卡片 */
            }

            /* 每一行的容器 */
            .row-container {
                display: flex;
                justify-content: center; 
                gap: 10px; /* 间隙调小 */
                width: 100%;
            }
        
            /* === 布局核心 === */
            
            /* 上面一行：2个视频 (每个约占 49%) */
            .row-top { 
                width: 49%; 
                min-width: 300px;
            }
        
            /* 下面一行：3个视频 (每个约占 32.5%) */
            /* 100% / 3 = 33.3%，减去间隙保留 32.5% */
            .row-bottom { 
                width: 32.5%; 
                min-width: 200px;
            }
        
            /* 视频标签通用样式 */
            video {
                width: 100%;
                display: block;
                border-radius: 6px;
                background: #000; 
                box-shadow: 0 4px 8px rgba(0,0,0,0.1);
                transition: transform 0.2s ease, box-shadow 0.2s ease;
            }
        
            video:hover {
                transform: translateY(-2px);
                box-shadow: 0 10px 20px rgba(0,0,0,0.15);
            }
        
            /* --- 范式标题 --- */
            .paradigm-title {
                width: 100%;
                text-align: center;
                font-size: 15px;
                text-transform: uppercase;
                letter-spacing: 1.5px;
                font-weight: 700;
                color: #007bff;
                background: rgba(0, 123, 255, 0.05);
                padding: 8px 0;
                margin: 15px 0 10px 0;
                border-radius: 4px;
            }
        
            .method-label {
                text-align: center;
                font-size: 15px;
                font-weight: 600;
                margin-bottom: 6px;
                color: #444;
            }
        
            /* --- 按钮组美化 --- */
            .scene-selector {
                display: flex;
                flex-wrap: wrap;
                justify-content: center;
                gap: 8px;
                margin-bottom: 30px;
            }
        
            .scene-btn {
                padding: 8px 20px;
                border: none;
                background-color: #f0f2f5;
                color: #555;
                font-size: 14px;
                font-weight: 600;
                border-radius: 50px;
                cursor: pointer;
                transition: all 0.2s ease;
                box-shadow: 0 2px 4px rgba(0,0,0,0.05);
            }
        
            .scene-btn:hover {
                background-color: #e4e6eb;
                transform: translateY(-1px);
            }
        
            .scene-btn.active {
                background-color: #007bff;
                color: white;
                box-shadow: 0 4px 10px rgba(0, 123, 255, 0.3);
                transform: scale(1.05);
            }
        
            /* 动画类 */
            .fade-in-anim {
                animation: quickFade 0.5s ease forwards;
            }
            @keyframes quickFade {
                from { opacity: 0.5; filter: blur(2px); }
                to { opacity: 1; filter: blur(0); }
            }
        </style>
        
        <div class="sub-section">
            <h1 class="text">Quantitative Comparisons</h1>
            <p class="text">
                Comparison with state-of-the-art methods. 
                <br><span style="font-size: 0.9em; color: #888;">Play, pause, or drag the progress bar on <em>any</em> video to control them all simultaneously!</span>
            </p>
        
            <div class="scene-selector" id="btn-group-1">
                <button class="scene-btn active" onclick="changeScene(1)">Scene 01</button>
                <button class="scene-btn" onclick="changeScene(2)">Scene 02</button>
                <button class="scene-btn" onclick="changeScene(3)">Scene 03</button>
                <button class="scene-btn" onclick="changeScene(4)">Scene 04</button>
                <button class="scene-btn" onclick="changeScene(5)">Scene 05</button>
                <button class="scene-btn" onclick="changeScene(6)">Scene 06</button>
                <button class="scene-btn" onclick="changeScene(7)">Scene 07</button>
                <button class="scene-btn" onclick="changeScene(8)">Scene 08</button>
                <button class="scene-btn" onclick="changeScene(9)">Scene 09</button>
                <button class="scene-btn" onclick="changeScene(10)">Scene 10</button>
            </div>
        
            <div class="video-container-card">
                <div class="video-grid" id="video-group-1">
                    
                    <div class="paradigm-title">Existing AE paradigm</div>
        
                    <div class="row-top">
                        <div class="method-label">HDRFlow</div>
                        <video class="sync-group-1" id="vid_hdrflow" autoplay loop muted playsinline controls>
                            <source src="demo/out01/output01_HDRFlow.mp4" type="video/mp4">
                        </video>
                    </div>
        
                    <div class="row-top">
                        <div class="method-label">LANHDR</div>
                        <video class="sync-group-1" id="vid_lanhdr" autoplay loop muted playsinline controls>
                            <source src="demo/out01/output01_LANHDR.mp4" type="video/mp4">
                        </video>
                    </div>
        
                    <div class="paradigm-title">Our DS paradigm</div>
        
                    <div class="row-bottom">
                        <div class="method-label">AHDNet</div>
                        <video class="sync-group-1" id="vid_ahdnet" autoplay loop muted playsinline controls>
                            <source src="demo/out01/output01_AHDNet.mp4" type="video/mp4">
                        </video>
                    </div>
        
                    <div class="row-bottom">
                        <div class="method-label">SCTNet</div>
                        <video class="sync-group-1" id="vid_sctnet" autoplay loop muted playsinline controls>
                            <source src="demo/out01/output01_SCTNet.mp4" type="video/mp4">
                        </video>
                    </div>
        
                    <div class="row-bottom">
                        <div class="method-label" style="color: #d63031;">EAFNet (Ours)</div>
                        <video class="sync-group-1" id="vid_pami" autoplay loop muted playsinline controls>
                            <source src="demo/out01/output01_pami.mp4" type="video/mp4">
                        </video>
                    </div>
        
                </div>
            </div>
        </div>
        
        
        <div class="sub-section" style="margin-top: 80px;">
            <h1 class="text">Real-world Comparisons</h1>
            <p class="text">
                Qualitative comparisons on real-world dynamic scenes.
            </p>
        
            <div class="scene-selector" id="btn-group-2">
                <button class="scene-btn real-btn active" onclick="changeRealScene(1)">Scene 01</button>
                <button class="scene-btn real-btn" onclick="changeRealScene(2)">Scene 02</button>
                <button class="scene-btn real-btn" onclick="changeRealScene(3)">Scene 03</button>
                <button class="scene-btn real-btn" onclick="changeRealScene(4)">Scene 04</button>
                <button class="scene-btn real-btn" onclick="changeRealScene(5)">Scene 05</button>
            </div>
        
            <div class="video-container-card">
                <div class="video-grid" id="video-group-2">
                    
                    <div class="row-top">
                        <div class="method-label" id="real_label_left">GT</div>
                        <video class="sync-group-2" id="vid_real_input" autoplay loop muted playsinline controls>
                            <source src="demo/real01/in01_pami2.mp4" type="video/mp4">
                        </video>
                    </div>
        
                    <div class="row-top">
                        <div class="method-label" style="color: #d63031;">Our Result</div>
                        <video class="sync-group-2" id="vid_real_output" autoplay loop muted playsinline controls>
                            <source src="demo/real01/out01_pami2.mp4" type="video/mp4">
                        </video>
                    </div>
        
                </div>
            </div>
        </div>
        
        
        <script>
            // 1. 视频同步播放逻辑
            function setupSynchronizedPlayback(className) {
                const videos = document.querySelectorAll('.' + className);
                let isSyncing = false;
        
                videos.forEach(video => {
                    video.addEventListener('play', () => {
                        if (!isSyncing) {
                            isSyncing = true;
                            videos.forEach(v => { if (v !== video) v.play(); });
                            isSyncing = false;
                        }
                    });
                    video.addEventListener('pause', () => {
                        if (!isSyncing) {
                            isSyncing = true;
                            videos.forEach(v => { if (v !== video) v.pause(); });
                            isSyncing = false;
                        }
                    });
                    video.addEventListener('seeking', () => {
                        if (!isSyncing) {
                            isSyncing = true;
                            videos.forEach(v => { 
                                if (v !== video && Math.abs(v.currentTime - video.currentTime) > 0.1) {
                                    v.currentTime = video.currentTime; 
                                }
                            });
                            isSyncing = false;
                        }
                    });
                });
            }
        
            document.addEventListener('DOMContentLoaded', () => {
                setupSynchronizedPlayback('sync-group-1');
                setupSynchronizedPlayback('sync-group-2');
            });
        
            // 2. 场景切换逻辑 (第一组)
            function changeScene(sceneNum) {
                updateActiveButton('btn-group-1', sceneNum);
                
                var sceneStr = sceneNum.toString().padStart(2, '0');
                var folderPath = "demo/out" + sceneStr + "/";
                var filePrefix = "output" + sceneStr + "_"; 
                
                var v_hdrflow = document.getElementById('vid_hdrflow');
                var v_lanhdr = document.getElementById('vid_lanhdr');
                var v_ahdnet = document.getElementById('vid_ahdnet');
                var v_sctnet = document.getElementById('vid_sctnet');
                var v_pami = document.getElementById('vid_pami');
                
                v_hdrflow.src = folderPath + filePrefix + "HDRFlow.mp4";
                v_lanhdr.src  = folderPath + filePrefix + "LANHDR.mp4";
                v_ahdnet.src  = folderPath + filePrefix + "AHDNet.mp4";
                v_sctnet.src  = folderPath + filePrefix + "SCTNet.mp4";
                v_pami.src    = folderPath + filePrefix + "pami.mp4";
        
                reloadAndPlay([v_hdrflow, v_lanhdr, v_ahdnet, v_sctnet, v_pami]);
                triggerAnimation('video-group-1');
            }
        
            // 3. 场景切换逻辑 (第二组 Real)
            function changeRealScene(sceneNum) {
                updateActiveButton('btn-group-2', sceneNum);
        
                var labelLeft = document.getElementById('real_label_left');
                labelLeft.innerText = (sceneNum === 1) ? "GT" : "Input (Reference)";
        
                var sceneStr = sceneNum.toString().padStart(2, '0');
                var folderPath = "demo/real" + sceneStr + "/";
                var inputFile  = "in" + sceneStr + "_pami2.mp4";
                var outputFile = "out" + sceneStr + "_pami2.mp4";
        
                var v_in = document.getElementById('vid_real_input');
                var v_out = document.getElementById('vid_real_output');
        
                v_in.src  = folderPath + inputFile;
                v_out.src = folderPath + outputFile;
        
                reloadAndPlay([v_in, v_out]);
                triggerAnimation('video-group-2');
            }
        
            // --- 辅助函数 ---
            function reloadAndPlay(videoElements) {
                videoElements.forEach(v => {
                    v.load();
                    var playPromise = v.play();
                    if (playPromise !== undefined) { playPromise.catch(error => {}); }
                });
            }
        
            function updateActiveButton(parentId, index) {
                var container = document.getElementById(parentId);
                var buttons = container.querySelectorAll('.scene-btn');
                buttons.forEach((btn, i) => {
                    if (i + 1 === index) {
                        btn.classList.add('active');
                    } else {
                        btn.classList.remove('active');
                    }
                });
            }
        
            function triggerAnimation(elementId) {
                var el = document.getElementById(elementId);
                el.classList.remove('fade-in-anim');
                void el.offsetWidth; 
                el.classList.add('fade-in-anim');
            }
        </script>















           

                <p class="text">
                    <strong>Intra-dataset evaluation: </strong>

                    <a href="#tab:hdr_comparison">Table 1</a> reports the intra-dataset evaluation results on Prabhakar’s and Kalantari’s datasets. 
                    Our EAFNet consistently achieves the best performance across both datasets. 
                    On the Kalantari dataset, it surpasses the second-best method by 0.08 dB in PSNR-μ, while on the Prabhakar dataset, the margin increases to 0.49 dB. 
                    These gains are complemented by improvements in SSIM-μ, indicating that our exposure-adaptive fusion strategy benefits both fidelity and structural consistency.

                <div id="tab:hdr_comparison" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                      <table class="data-table">
                        <thead>
                          <tr>
                            <th colspan="2" class="tb-hdr"></th>
                            <th colspan="5" class="tb-hdr">train and test on Kalantari’s dataset</th>
                            <th colspan="5" class="tb-hdr">train and test on Prabhakar’s dataset</th>
                          </tr>
                          <tr>
                            <th class="section-border">Method</th>
                            <th class="section-border">Metrics</th>
                            <th>PSNR-μ (↑)</th>
                            <th>PSNR-L (↑)</th>
                            <th>SSIM-μ (↑)</th>
                            <th>SSIM-L (↑)</th>
                            <th class="section-border">HDR-VDP-2 (↑)</th>
                            <th>PSNR-μ (↑)</th>
                            <th>PSNR-L (↑)</th>
                            <th>SSIM-μ (↑)</th>
                            <th>SSIM-L (↑)</th>
                            <th>HDR-VDP-2 (↑)</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td colspan="2" class="section-border">Kalantari (CGF 2017)</td>
                            <td>42.74</td>
                            <td>41.22</td>
                            <td>0.9877</td>
                            <td>0.9848</td>
                            <td class="section-border">60.51</td>
                            <td>35.63</td>
                            <td>32.50</td>
                            <td>0.09613</td>
                            <td>0.9692</td>
                            <td>59.42</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">AHDRNet (CVPR 2019)</td>
                            <td>43.77</td>
                            <td>41.35</td>
                            <td>0.9907</td>
                            <td>0.9859</td>
                            <td class="section-border">62.30</td>
                            <td>38.61</td>
                            <td>35.26</td>
                            <td>0.9663</td>
                            <td>0.9794</td>
                            <td>61.14</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">Prabhakar (ECCV 2020)</td>
                            <td>43.08</td>
                            <td>41.68</td>
                            <td>-</td>
                            <td>-</td>
                            <td class="section-border">62.21</td>
                            <td>38.30</td>
                            <td>34.98</td>
                            <td>0.9702</td>
                            <td>0.9781</td>
                            <td>-</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">HDR-Trans (ECCV 2022)</td>
                            <td>44.28</td>
                            <td class="secondbest">42.88</td>
                            <td>0.9916</td>
                            <td>0.9884</td>
                            <td>66.03</td>
                            <td class="secondbest">41.31</td>
                            <td class="secondbest">39.44</td>
                            <td class="secondbest">0.9726</td>
                            <td class="secondbest">0.9885</td>
                            <td class="secondbest">63.01</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">DomainPlus (MM 2022)</td>
                            <td>44.02</td>
                            <td>41.28</td>
                            <td>0.9910</td>
                            <td>0.9864</td>
                            <td class="section-border">62.91</td>
                            <td>40.38</td>
                            <td>38.08</td>
                            <td>0.9698</td>
                            <td>0.9872</td>
                            <td>62.12</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">SCTNet (ICCV 2023)</td>
                            <td>44.13</td>
                            <td>42.12</td>
                            <td>0.9916</td>
                            <td>0.9890</td>
                            <td class="section-border">66.65</td>
                            <td>41.23</td>
                            <td>38.75</td>
                            <td>0.9724</td>
                            <td>0.9881</td>
                            <td>62.29</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">SAFNet (ECCV 2024)</td>
                            <td class="secondbest">44.61</td>
                            <td class="best">43.09</td>
                            <td class="secondbest">0.9918</td>
                            <td class="secondbest">0.9892</td>
                            <td class="secondbest">66.93</td>
                            <td>40.18</td>
                            <td>37.90</td>
                            <td>0.9705</td>
                            <td>0.9865</td>
                            <td>62.04</td>
                          </tr>
                          <tr style="background-color:#FFEEED;">
                            <td colspan="2" class="section-border">EAFNet (Ours)</td>
                            <td class="best">44.69</td>
                            <td>42.19</td>
                            <td class="best">0.9920</td>
                            <td class="best">0.9895</td>
                            <td class="best">68.35</td>
                            <td class="best">41.80</td>
                            <td class="best">40.13</td>
                            <td class="best">0.9731</td>
                            <td class="best">0.9895</td>
                            <td class="best">63.53</td>
                          </tr>
                        </tbody>
                      </table>
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                      Table1: Comparison results on Kalantari’s dataset and Prabhakar’s dataset. Best results are <span class="best">bold red</span>, second-best are <span class="secondbest">underlined blue</span>.
                    </figcaption>
                  </div>
                  


                  <p class="text">
                    <strong>Cross-dataset evaluation: </strong>

                    We further conduct cross-dataset validation, where training and testing are performed on different datasets (<a href="#tab:cross_dataset">Table 2</a>). 
                    Our EAFNet maintains clear superiority in this challenging setting, with cross-domain gains even larger than in the intra-dataset case. 
                    This demonstrates that our model does not overfit to dataset-specific statistics, but learns exposure-aware and motion-robust fusion representations that transfer effectively across domains. 
                    The strong bidirectional results confirm the generality and domain-agnostic nature of our fusion mechanism.
                

                  <div id="tab:cross_dataset" style="display: flex; flex-direction: column; align-items: center;">
                    <div class="table-container">
                      <table class="data-table">
                        <thead>
                          <tr>
                            <th colspan="2" class="tb-hdr"></th>
                            <th colspan="4" class="tb-hdr">train on Kalantari’s dataset, test on Prabhakar’s dataset</th>
                            <th colspan="4" class="tb-hdr">train on Prabhakar’s dataset, test on Kalantari’s dataset</th>
                          </tr>
                          <tr>
                            <th class="section-border">Method</th>
                            <th class="section-border">Metrics</th>
                            <th>PSNR-μ (↑)</th>
                            <th>PSNR-L (↑)</th>
                            <th>SSIM-μ (↑)</th>
                            <th>SSIM-L (↑)</th>
                            <th>PSNR-μ (↑)</th>
                            <th>PSNR-L (↑)</th>
                            <th>SSIM-μ (↑)</th>
                            <th>SSIM-L (↑)</th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr>
                            <td colspan="2" class="section-border">AHDRNet (CVPR 2019)</td>
                            <td>33.96</td>
                            <td>32.46</td>
                            <td>0.9601</td>
                            <td>0.9542</td>
                            <td>40.03</td>
                            <td>36.71</td>
                            <td>0.9855</td>
                            <td>0.9758</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">HDR-Trans (ECCV 2022)</td>
                            <td>34.07</td>
                            <td>36.62</td>
                            <td>0.9675</td>
                            <td>0.9656</td>
                            <td class="secondbest">41.38</td>
                            <td class="secondbest">39.21</td>
                            <td>0.9890</td>
                            <td class="best">0.9873</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">DomainPlus (MM 2022)</td>
                            <td>32.64</td>
                            <td>30.42</td>
                            <td>0.9046</td>
                            <td>0.9074</td>
                            <td>41.15</td>
                            <td>38.18</td>
                            <td>0.9873</td>
                            <td>0.9837</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">SCTNet (ICCV 2023)</td>
                            <td>33.83</td>
                            <td>30.95</td>
                            <td>0.9584</td>
                            <td>0.9521</td>
                            <td>40.88</td>
                            <td>37.59</td>
                            <td>0.9892</td>
                            <td>0.9842</td>
                          </tr>
                          <tr>
                            <td colspan="2" class="section-border">SAFNet (ECCV 2024)</td>
                            <td class="secondbest">38.00</td>
                            <td class="secondbest">34.65</td>
                            <td>0.9597</td>
                            <td class="secondbest">0.9793</td>
                            <td>40.86</td>
                            <td>37.50</td>
                            <td>0.9882</td>
                            <td>0.9810</td>
                          </tr>
                          <tr style="background-color:#FFEEED;">
                            <td colspan="2" class="section-border">EAFNet (Ours)</td>
                            <td class="best">39.26</td>
                            <td class="best">35.99</td>
                            <td class="best">0.9707</td>
                            <td class="best">0.9848</td>
                            <td class="best">42.02</td>
                            <td class="best">39.38</td>
                            <td class="best">0.9903</td>
                            <td class="secondbest">0.9870</td>
                          </tr>
                        </tbody>
                      </table>
                    </div>
                    <figcaption style="text-align: center; width: 140%;">
                      Table2: Cross-dataset evaluation on Kalantari’s dataset and Prabhakar’s dataset. Best results are <span class="best">bold red</span>, second-best are <span class="secondbest">underlined blue</span>.
                    </figcaption>
                  </div>
                

        </div>
        </div>

        <div id='Paradigm' class="connector-block">

            <h1 class="text">dual-stream HDR video generation paradigm</h1>
            <p class="text">
                
                We introduce a dual-stream HDR video generation paradigm that explicitly decouples temporal luminance anchoring from exposure-variant detail reconstruction. Our approach employs a fixed-exposure stream to maintain temporal alignment across frames, while a complementary stream with varying exposures enhances the dynamic range. This design fundamentally improves temporal consistency and reconstruction stability.
            </p>
            <d-figure id="introduction3">
                <figure>
                    <img data-zoomable="" draggable="false" src="static/img/introduction3.png" alt="Spatial Vision Aggregator (SVA)">
                    <figcaption>
                        <strong>Figure 1:</strong> Overview of challenges in the alternating exposure (AE) paradigm and our proposed dual-stream paradigm for stable HDR video generation.
                    </figcaption>
                </figure>
            </d-figure>
        </div>

        <div id="System" class="data-block">
            <h1 class="text">dual-camera system</h1>
            <p class="text">
                We design and implement an asynchronous dual-camera system to validate the feasibility of our proposed solution and bridge the gap between algorithmic design and practical deployment. 
                Unlike traditional synchronized setups constrained by long-exposure frames, our system enables high-frame-rate video capture in dynamic scenes by supporting independent exposure control without requiring hardware-level synchronization. 
                Moreover, the system seamlessly integrates with existing image deghosting methods to achieve temporally consistent reconstruction.

            </p>

            <div class="subsection">
                <p class="text" id="data_collection">
                    <strong>Video Capture and System Design: </strong>
                    Our dual-camera system uses two identical cameras with resolution \(w \times h\). One camera records a medium-exposure reference sequence \(L_{ref}(t_1)\), 
                    while the other alternates between low and high exposures \(L_{non\mbox{-}ref}(t_2)\) for dynamic range expansion.
                    We pair the low/high exposure frames with nearby reference frames using timestamp metadata.The input groups are then processed by the network to reconstruct HDR video at the same frame rate as \(L_{ref}(t_1)\).
                </p>
                <d-figure id="fig-cambrian7m" style="text-align: center;">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/dual_system.png" alt="Cambrian-7M: A Large-Scale Curated Instruction Tuning Dataset for Training MLLM" style="width: 500px; height: auto;">
                        <figcaption>
                            <strong>Figure 2:</strong> Visualization of our dual-camera system. The primary camera captures continuous medium-exposure sequences as reference for temporal consistency, while the secondary camera alternates between low- and high-exposure to provide complementary information for reconstruction.
                        </figcaption>
                    </figure>
                </d-figure>



                <p class="text">
                    <strong>dataset</strong>
                    to do 
                </p>
 
            </div>

    

            <div id='Method' class="connector-block">

                <h1 class="text">Our exposure-adaptive fusion network</h1>
                <p class="text">
                       
                </p>
                <d-figure id="model1">
                    <figure>
                        <img data-zoomable="" draggable="false" src="static/img/model1.png" alt="Spatial Vision Aggregator (SVA)">
                        <figcaption>
                            <strong>Figure 3:</strong> The architecture of EAFNet consists of a pre-alignment subnetwork, an asymmetric cross-feature fusion subnetwork, and a restoration subnetwork. We introduce GLA and EFSM to leverage exposure information, explore the intrinsic properties of the images, and help preserve finer details across varying exposures. The asymmetric cross-feature fusion subnetwork improves image fusion by aligning cross-scale features and performing cross-feature fusion. The reconstruction subnetwork adopts a multi-scale architecture to reduce ghosting and refine features at different resolutions.
                        </figcaption>
                    </figure>
                </d-figure>
            </div>





        <div id="conclusion" style="position: relative; margin-top: 40px; margin-bottom: 0px;">
            <h2 class="text" style="margin-top:0px; margin-bottom:10px">Conclusion</h2>
            <p class="text">
                In this work, we revisited the fundamental cause of temporal instability in alternating-exposure (AE) HDR video, 
                which lies in the entanglement of temporal luminance anchoring with exposure-dependent detail selection, 
                and proposed a dual-stream paradigm that explicitly decouples these two roles. Extensive experiments on multiple datasets and real-world sequences demonstrate that the proposed paradigm improves both temporal stability and reconstruction quality compared with AE-based baselines, while remaining cost-efficient and deployment-friendly.
                Our dual-camera framework provides a promising direction for real-time HDR video capture.
            </p>
        </div>


        </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @article{zhang2025capturing,<br>
                &nbsp;&nbsp;title={Capturing Stable HDR Videos Using a Dual-Camera System},<br>
                &nbsp;&nbsp;author={Zhang, Qianyu and Zheng, Bolun and Pan, Hangjia and Zhu, Lingyu and Zhu, Zunjie and Li, Zongpeng and Wang, Shiqi},<br>
                &nbsp;&nbsp;journal={arXiv preprint arXiv:2507.06593},<br>
                &nbsp;&nbsp;year={2025}<br>
                }
            </p>

            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>
        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
        <d-bibliography src="bibliography.bib"></d-bibliography>
        <script src="./static/js/nav-bar.js"></script>
    </body>
</html>
